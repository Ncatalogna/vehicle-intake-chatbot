# Demo Challenge: Agentes y Workflows con LangGraph - Vehicle Intake Chatbot

Este repositorio contiene una demostraci贸n t茅cnica que explora la implementaci贸n de un sistema de agentes conversacionales y workflows utilizando **LangChain** y **LangGraph**. El objetivo principal es mostrar c贸mo orquestar flujos de conversaci贸n complejos de manera modular y robusta.

El proyecto se divide en dos componentes principales:

1.  **`/chatbot`**: Un asistente conversacional que gu铆a a los usuarios a trav茅s de un proceso de admisi贸n vehicular.
2.  **`/api`**: Un backend de servicios RESTful construido con **FastAPI** que maneja la l贸gica de negocio y la persistencia de datos.

Para una explicaci贸n detallada de cada componente, por favor, consulta sus respectivos `README.md`:

  - **[Documentaci贸n del Chatbot](./chatbot/README.md)**
  - **[Documentaci贸n de la API](./api/README.md)**

##  Entorno de Desarrollo (Recomendado)

La forma m谩s sencilla de levantar y probar este proyecto es utilizando el **Dev Container** incluido, que garantiza un entorno de desarrollo consistente y autocontenido.

El Dev Container utiliza Docker Compose para orquestar los servicios necesarios:

  * **`app`**: Un contenedor con Python 3.12 y todas las dependencias del proyecto preinstaladas en un entorno virtual.
  * **`db`**: Un servicio de base de datos **PostgreSQL** con la extensi贸n `pgvector` habilitada, listo para ser utilizado por la API.

### C贸mo usar el Dev Container

1.  Abre el proyecto en **Visual Studio Code**.
2.  Aseg煤rate de tener instalada la extensi贸n [Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers).
3.  VS Code detectar谩 la configuraci贸n y te pedir谩 **"Reopen in Container"**. Haz clic para aceptar.
4.  Una vez dentro, podr谩s ejecutar la API y el Chatbot desde la terminal de VS Code como se describe en sus respectivos `README.md`.

Si prefieres una configuraci贸n manual, por favor, sigue las gu铆as detalladas en la documentaci贸n de cada subdirectorio.

## C贸mo ejecutar la aplicaci贸n

Si no se utiliza el devcontainer, se deben seguir los siguientes pasos:
1. Montar la base de datos con `docker-compose up db`.
2. Instalar las dependencias con `pip install -r requirements.txt`.
3. Configurar el archivo `.env` en la carpeta `api`.
4. Ejecutar las migraciones de la base de datos.
5. Ejecutar la API.
6. Configurar el archivo `.env` en la carpeta `chatbot`.
7. Ejecutar el chatbot.

## Pruebas HTTP

En la carpeta `.rest` se encuentran los archivos `clients.rest`, `eligibility.rest` y `vehicles.rest`. Estos archivos contienen pruebas HTTP que se pueden ejecutar con la extensi贸n de Visual Studio Code [REST Client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client) para probar los endpoints de la API.

-----

##  Soluciones de Compromiso (Trade-offs)

Durante el desarrollo, se tomaron ciertas decisiones para mitigar limitaciones encontradas en las librer铆as:

  - **Serializaci贸n en LangGraph**: Se detectaron problemas de estabilidad al usar tipos `Enum` y `List` en los esquemas Pydantic para la salida estructurada (`with_structured_output`). Estos problemas, documentados en la comunidad de LangChain, pod铆an causar fallos inesperados.
      - *LangChain Issue #33444*: Reporta un crash (`AttributeError: 'int' object has no attribute 'name'`) por un valor de `FinishReason` no reconocido de la API de Gemini.
      - *LangChain Discussion #28778*: Los usuarios confirman que el uso de `Enum` en la especificaci贸n de salida estructurada causa.
  - **Mitigaci贸n**: Para asegurar la robustez, se opt贸 por usar tipos m谩s simples como `str` en el esquema Pydantic y manejar la validaci贸n y conversi贸n a `Enum` en el c贸digo de la aplicaci贸n. Esto evita delegar la restricci贸n al LLM, que puede no ser consistente.

-----

##  Deuda T茅cnica

  - **Flujo de Agente m谩s Din谩mico**: El flujo actual, aunque orquestado, sigue un patr贸n relativamente estructurado. Una mejora ser铆a hacerlo menos dependiente de un RAG (Retrieval-Augmented Generation) impl铆cito y m谩s capaz de decidir rutas complejas de manera aut贸noma.

-----

##  Posibles Implementaciones

Debido a limitaciones de tiempo, las siguientes caracter铆sticas no fueron implementadas pero representan los siguientes pasos l贸gicos para el proyecto:

  - **Optimizaci贸n del Flujo con un Patr贸n "Extractor-Ejecutor"**: El workflow actual se basa en agentes que interpretan la intenci贸n y ejecutan herramientas (tools) que a su vez llaman a la API. Para un flujo tan definido como este, se podr铆a implementar un patr贸n m谩s directo y eficiente en el uso de tokens. En lugar de un agente, el LLM se usar铆a 煤nicamente para extraer la informaci贸n del usuario en un formato estructurado (JSON). El propio nodo del grafo, al recibir estos datos, ser铆a el encargado de ejecutar la l贸gica y realizar las llamadas a la API directamente con c贸digo Python. Este enfoque, similar a un RAG pero centrado en la acci贸n, reducir铆a la sobrecarga de tokens y la complejidad de la interacci贸n del LLM con las herramientas.
  - **Monitoreo con Langfuse**: Integrar Langfuse para obtener trazabilidad, monitoreo y an谩lisis detallado de las interacciones y el rendimiento de los agentes.
  - **B煤squeda Vectorial con pgvector**: Implementar una b煤squeda de similitud sem谩ntica (por ejemplo, para encontrar clientes o veh铆culos con datos parecidos) aprovechando la extensi贸n `pgvector` ya incluida en la base de datos.
  - **Sub-Workflows en LangGraph**: Refactorizar el grafo principal para utilizar sub-grafos (o "sub-workflows"). Esto permitir铆a encapsular l贸gicas complejas (como todo el proceso de validaci贸n de cliente) en workflows anidados, haciendo el sistema principal m谩s limpio y modular.